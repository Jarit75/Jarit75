<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Talking Avatar with LLM</title>
    <script src="https://cdn.babylonjs.com/babylon.js"></script>
    <script src="https://cdn.babylonjs.com/loaders/babylonjs.loaders.min.js"></script>
    <script src="https://readyplayer.me/assets/iframe-api.js"></script>
    <style>
        body {
            margin: 0;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            height: 100vh;
            overflow: hidden;
        }
        canvas {
            display: block;
            width: 800px;
            height: 600px;
        }
        #input-container {
            margin-top: 10px;
            text-align: center;
            width: 100%;
        }
        #input-field {
            width: 60%;
            padding: 10px;
            font-size: 16px;
        }
		#button {
            padding: 10px 20px;
            font-size: 16px;
            margin-left: 10px;
            cursor: pointer;
        }
    </style>
</head>
<body>
    <div id="avatar-container"></div>
    <div id="input-container">
        <input type="text" id="userText" placeholder="Type something...">
		<button type="button" onclick="sendMessage()">Send</button>
		<button id= "speach"  type="button" onclick="playaudio()">PlayAudio</button>
    </div>
    <script>
        // Babylon.js basic setup
		
        const canvas = document.createElement('canvas');
        document.getElementById('avatar-container').appendChild(canvas);

        const engine = new BABYLON.Engine(canvas, true);
        const scene = new BABYLON.Scene(engine);
		
        
        // Camera
        const camera = new BABYLON.ArcRotateCamera("Camera", Math.PI / 2, Math.PI / 3, 3, BABYLON.Vector3.Zero(), scene);
		camera.wheelDeltaPercentage = 0.002
        camera.attachControl(canvas, true);

		camera.setPosition(new BABYLON.Vector3(0.050960765715934594, 2, 1.227852849463614));
		
		
		
        // Lighting
        const light = new BABYLON.HemisphericLight("light", new BABYLON.Vector3(1, 1, 0), scene);

        // Load Ready Player Me Avatar
        const avatarUrl = "https://models.readyplayer.me/66f95c77abf3775636daa1fa.glb";
        let avatarMesh;


		    
			const baseUrl = "https://raw.githubusercontent.com/Jarit75/Jarit75/main/";

            // ready player me model, rig matches mixamo animations
            const rpm = BABYLON.SceneLoader.LoadAssetContainer("https://raw.githubusercontent.com/Jarit75/Jarit75/main/", "pretty_boy.glb", scene, assets => {
            assets.addAllToScene();
			
			avatarMesh = scene; // Adjust this if needed
			console.log("Avatar loaded");
            // mixamo animation
            const hiphop = BABYLON.SceneLoader.ImportAnimations(baseUrl, "DPM-X.glb", scene, false,
                BABYLON.SceneLoaderAnimationGroupLoadingMode.Clean, null);
			});
       // });
		

        // Render loop
        engine.runRenderLoop(() => {
            scene.render();

        });

        window.addEventListener("resize", () => {
            engine.resize();
        });

       // Lipsync Animation
    function animateMouth(analyzer, frequenzy, jawValue) {

		console.log('here1');
        analyzer.getByteFrequencyData(frequenzy);
	
        const amplitude = frequenzy.reduce((sum, value) => sum + value) / frequenzy.length;

        
		jawValue = (Math.random() * 2) + amplitude / 512;
		
        const mouthMesh = avatarMesh.getMeshByName("Wolf3D_Head").morphTargetManager.getTargetByName("mouthOpen");
		//console.log('jaw is:',jawValue);
        if (mouthMesh) {
		   console.log('open mouth');
		   avatarMesh.getMeshByName("Wolf3D_Head").morphTargetManager.getTargetByName("mouthOpen").influence = jawValue;
        }
      }

        // Text-to-voice integration with Eleven Labs
		let audioglobal;
        async function triggerElevenLabsTTS(inputText) {
		 
            const response = await fetch('https://api.elevenlabs.io/v1/text-to-speech/o2EFJrhd0KKA5rzXa3Y0', {
                method: 'POST',
                headers: {
                    'xi-api-key': '36481a866ba24ebc9655b3ab03dc2c83',
                    'Content-Type': 'application/json',
                    'Accept': 'audio/mpeg'
                },
                body: JSON.stringify({
                    "model_id": "eleven_multilingual_v2",
                    "similarity_boost": 85,
                    "stability": 40,
                    "style": 20,
                    "text": inputText
                })
            });
            
			
            const audioBlob = await response.blob();
            const audioURL = URL.createObjectURL(audioBlob);
			console.log('audioURL' , audioURL);
            audioglobal = new Audio(audioURL);
            
			
          
          }
		// Function to play the audio and enable voice recognition after it finishes
        async function playaudio() {
            console.log('Playing TTS audio');
			const jawValue = 0;
			
            // Setup AudioContext and AnalyserNode
            audioContext = new (window.AudioContext || window.webkitAudioContext)();
            const source = audioContext.createMediaElementSource(audioglobal);
            

		    analyser = audioContext.createAnalyser();
            source.connect(analyser);
            analyser.connect(audioContext.destination);
            frequencyData = new Uint8Array(analyser.frequencyBinCount);
			
            // Play audio and animate mouth
           audioglobal.play();
           const lipsyncInterval = setInterval(() => {
            if (audioglobal.paused || audioglobal.ended) {
                clearInterval(lipsyncInterval);
            } else {
			    console.log('do animate mouth');
                animateMouth(analyser, frequencyData, jawValue);
            }
           }, 50);
		   
        }

        // LLM REST call integration
        async function queryLLM(prompt) {
		    
            const response = await fetch('https://openrouterfull.us-e2.cloudhub.io/utter', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify({ prompt, 
                                       "character": "Prettyboy",
                                       "model": "meta-llama/llama-3-70b-instruct" }),
            });

            const result = await response.json();
			console.log('result:', result.message.content);
            return result.message.content;
        }




        async function sendMessage() {
            const userInput = document.getElementById('userText').value;
            if (!userInput) return;

            //appendMessage('user', userInput);

            try {
                const Llmresponse = await queryLLM(userInput);
				console.log('LLM response ok');
                //appendMessage('llm', response);
				console.log('call elevenlabs');
                triggerElevenLabsTTS(Llmresponse); // Call ElevenLabs TTS for the response
            } catch (error) {
                //appendMessage('llm', 'Error occurred.');
				console.log('LLM error:');
            }

            document.getElementById('userText').value = ''; // Clear the input field
        }
    </script>
</body>
</html>
